{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computational Graphs in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the required packages\n",
    "import torch\n",
    "from IPython.display import display, Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the graph a,b,c,d are leaf nodes and e is the root node\n",
    "# The graph is constructed with every line since the \n",
    "# computational graphs are dynamic in PyTorch\n",
    "\n",
    "a = torch.tensor([2.0],requires_grad=True)\n",
    "b = torch.tensor([3.0],requires_grad=True)\n",
    "c = torch.tensor([5.0],requires_grad=True)\n",
    "d = torch.tensor([10.0],requires_grad=True)\n",
    "u = a*b\n",
    "t = torch.log(d)\n",
    "v = t*c\n",
    "t.retain_grad() \n",
    "e = u+v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.is_leaf: True\n",
      "a.grad_fn: None\n",
      "a.grad: None\n",
      "\n",
      "b.is_leaf: True\n",
      "b.grad_fn: None\n",
      "b.grad: None\n",
      "\n",
      "c.is_leaf: True\n",
      "c.grad_fn: None\n",
      "c.grad: None\n",
      "\n",
      "d.is_leaf: True\n",
      "d.grad_fn: None\n",
      "d.grad: None\n",
      "\n",
      "e.is_leaf: False\n",
      "e.grad_fn: <AddBackward0 object at 0x0000020EB13FAFD0>\n",
      "e.grad: None\n",
      "\n",
      "u.is_leaf: False\n",
      "u.grad_fn: <MulBackward0 object at 0x0000020EB13FAFD0>\n",
      "u.grad: None\n",
      "\n",
      "v.is_leaf: False\n",
      "v.grad_fn: <MulBackward0 object at 0x0000020EB13FAFD0>\n",
      "v.grad: None\n",
      "\n",
      "t.is_leaf: False\n",
      "t.grad_fn: <LogBackward object at 0x0000020EB13FAFD0>\n",
      "t.grad: None\n"
     ]
    }
   ],
   "source": [
    "print(f'a.is_leaf: {a.is_leaf}')\n",
    "print(f'a.grad_fn: {a.grad_fn}')\n",
    "print(f'a.grad: {a.grad}')\n",
    "print()\n",
    "\n",
    "print(f'b.is_leaf: {b.is_leaf}')\n",
    "print(f'b.grad_fn: {b.grad_fn}')\n",
    "print(f'b.grad: {b.grad}')\n",
    "print()\n",
    "\n",
    "print(f'c.is_leaf: {c.is_leaf}')\n",
    "print(f'c.grad_fn: {c.grad_fn}')\n",
    "print(f'c.grad: {c.grad}')\n",
    "print()\n",
    "\n",
    "print(f'd.is_leaf: {d.is_leaf}')\n",
    "print(f'd.grad_fn: {d.grad_fn}')\n",
    "print(f'd.grad: {d.grad}')\n",
    "print()\n",
    "\n",
    "print(f'e.is_leaf: {e.is_leaf}')\n",
    "print(f'e.grad_fn: {e.grad_fn}')\n",
    "print(f'e.grad: {e.grad}')\n",
    "print()\n",
    "\n",
    "print(f'u.is_leaf: {u.is_leaf}')\n",
    "print(f'u.grad_fn: {u.grad_fn}')\n",
    "print(f'u.grad: {u.grad}')\n",
    "print()\n",
    "\n",
    "print(f'v.is_leaf: {v.is_leaf}')\n",
    "print(f'v.grad_fn: {v.grad_fn}')\n",
    "print(f'v.grad: {v.grad}')\n",
    "print()\n",
    "\n",
    "print(f't.is_leaf: {t.is_leaf}')\n",
    "print(f't.grad_fn: {t.grad_fn}')\n",
    "print(f't.grad: {t.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to call the backward function again you'll have to \n",
    "# set retain_graph = True\n",
    "e.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Since retain_grad() was called for node t, gradients were \n",
    "# calculated despite it not being a leaf node.\n",
    "print(t.grad)\n",
    "# Since retain_grad() was not called for node u and u is not a leaf node, \n",
    "# gradients were not calculated for this node\n",
    "print(u.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.is_leaf: True\n",
      "a.grad_fn: None\n",
      "a.grad: tensor([3.])\n",
      "\n",
      "b.is_leaf: True\n",
      "b.grad_fn: None\n",
      "b.grad: tensor([2.])\n",
      "\n",
      "c.is_leaf: True\n",
      "c.grad_fn: None\n",
      "c.grad: tensor([2.3026])\n",
      "\n",
      "d.is_leaf: True\n",
      "d.grad_fn: None\n",
      "d.grad: tensor([0.5000])\n",
      "\n",
      "e.is_leaf: False\n",
      "e.grad_fn: <AddBackward0 object at 0x0000020EAF69DCF8>\n",
      "e.grad: None\n",
      "\n",
      "u.is_leaf: False\n",
      "u.grad_fn: <MulBackward0 object at 0x0000020EAF69DCF8>\n",
      "u.grad: None\n",
      "\n",
      "v.is_leaf: False\n",
      "v.grad_fn: <MulBackward0 object at 0x0000020EAF69DCF8>\n",
      "v.grad: None\n",
      "\n",
      "t.is_leaf: False\n",
      "t.grad_fn: <LogBackward object at 0x0000020EAF69DCF8>\n",
      "t.grad: tensor([5.])\n"
     ]
    }
   ],
   "source": [
    "print(f'a.is_leaf: {a.is_leaf}')\n",
    "print(f'a.grad_fn: {a.grad_fn}')\n",
    "print(f'a.grad: {a.grad}')\n",
    "print()\n",
    "\n",
    "print(f'b.is_leaf: {b.is_leaf}')\n",
    "print(f'b.grad_fn: {b.grad_fn}')\n",
    "print(f'b.grad: {b.grad}')\n",
    "print()\n",
    "\n",
    "print(f'c.is_leaf: {c.is_leaf}')\n",
    "print(f'c.grad_fn: {c.grad_fn}')\n",
    "print(f'c.grad: {c.grad}')\n",
    "print()\n",
    "\n",
    "print(f'd.is_leaf: {d.is_leaf}')\n",
    "print(f'd.grad_fn: {d.grad_fn}')\n",
    "print(f'd.grad: {d.grad}')\n",
    "print()\n",
    "\n",
    "print(f'e.is_leaf: {e.is_leaf}')\n",
    "print(f'e.grad_fn: {e.grad_fn}')\n",
    "print(f'e.grad: {e.grad}')\n",
    "print()\n",
    "\n",
    "print(f'u.is_leaf: {u.is_leaf}')\n",
    "print(f'u.grad_fn: {u.grad_fn}')\n",
    "print(f'u.grad: {u.grad}')\n",
    "print()\n",
    "\n",
    "print(f'v.is_leaf: {v.is_leaf}')\n",
    "print(f'v.grad_fn: {v.grad_fn}')\n",
    "print(f'v.grad: {v.grad}')\n",
    "print()\n",
    "\n",
    "print(f't.is_leaf: {t.is_leaf}')\n",
    "print(f't.grad_fn: {t.grad_fn}')\n",
    "print(f't.grad: {t.grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients computed using the PyTorch autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{\\partial e}{\\partial a} = 3.0$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{\\partial e}{\\partial b} = 2.0$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{\\partial e}{\\partial c} = 2.3025851249694824$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{\\partial e}{\\partial d} = 0.5$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Math(fr'\\frac{{\\partial e}}{{\\partial a}} = {a.grad.item()}'))\n",
    "print()\n",
    "display(Math(fr'\\frac{{\\partial e}}{{\\partial b}} = {b.grad.item()}'))\n",
    "print()\n",
    "display(Math(fr'\\frac{{\\partial e}}{{\\partial c}} = {c.grad.item()}'))\n",
    "print()\n",
    "display(Math(fr'\\frac{{\\partial e}}{{\\partial d}} = {d.grad.item()}'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
